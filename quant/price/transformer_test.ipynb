{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81ee890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# import pickle5 as pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a02be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.read_csv('../price_universe.csv', index_col=0)\n",
    "price.index = pd.to_datetime(price.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "883e48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-01-02    2323.76\n",
       "2018-01-03    2339.29\n",
       "2018-01-04    2350.30\n",
       "2018-01-05    2366.48\n",
       "2018-01-08    2370.14\n",
       "               ...   \n",
       "2021-12-23    4801.71\n",
       "2021-12-27    4869.42\n",
       "2021-12-28    4865.61\n",
       "2021-12-29    4871.72\n",
       "2021-12-30    4859.24\n",
       "Name: S&P 500, Length: 956, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snp = price['S&P 500']\n",
    "snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4847631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "snp = min_max_scaler.fit_transform(snp.to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a8d03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snp = price.iloc[:, :1]\n",
    "# train = snp[:-30]\n",
    "# data_train = train.to_numpy()\n",
    "# data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f6b3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = snp[-30:]\n",
    "# test\n",
    "# data_test = test.to_numpy()\n",
    "# data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4923b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_scaler = MinMaxScaler()\n",
    "# data_train = min_max_scaler.fit_transform(train.to_numpy().reshape(-1,1))\n",
    "# data_test = min_max_scaler.transform(test.to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad99f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Transformer Model\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, ffn_hidden_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(input_size, num_heads, ffn_hidden_size, dropout_rate)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = n # n is the number of features in your dataset\n",
    "hidden_size = 128\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "ffn_hidden_size = 128\n",
    "dropout_rate = 0.1\n",
    "model = StockTransformer(input_size, hidden_size, num_layers, num_heads, ffn_hidden_size, dropout_rate)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05f9f842",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Transformer' from 'tensorflow.keras.layers' (C:\\Users\\bongkyun\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\api\\_v2\\keras\\layers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13052\\2791814807.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Transformer' from 'tensorflow.keras.layers' (C:\\Users\\bongkyun\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\api\\_v2\\keras\\layers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Transformer, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the S&P 500 data into a pandas DataFrame\n",
    "data = pd.read_csv('../price_universe.csv', index_col=0)\n",
    "\n",
    "# Prepare the input data for the model\n",
    "input_data = price['S&P 500']\n",
    "\n",
    "# Define the Transformer model\n",
    "input_layer = Input(shape=(input_data.shape[1],))\n",
    "transformer = Transformer(num_heads=8, d_model=256, dff=1024, num_layers=2, activation=\"relu\")(input_layer)\n",
    "output_layer = Dense(units=1, activation=\"linear\")(transformer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Train the model on the input data\n",
    "model.fit(input_data, data[\"Close\"], epochs=100, batch_size=32)\n",
    "\n",
    "# Use the trained model to make predictions on new data\n",
    "new_data = np.array([[100, 102, 98, 101]])\n",
    "prediction = model.predict(new_data)\n",
    "print(\"The predicted stock price for the new data is:\", prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9ea06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
