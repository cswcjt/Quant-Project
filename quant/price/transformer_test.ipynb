{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81ee890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# import pickle5 as pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a02be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.read_csv('../price_universe.csv', index_col=0)\n",
    "price.index = pd.to_datetime(price.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07071d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>A</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACGL</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>...</th>\n",
       "      <th>SB</th>\n",
       "      <th>CC</th>\n",
       "      <th>KC</th>\n",
       "      <th>CT</th>\n",
       "      <th>LC</th>\n",
       "      <th>LH.1</th>\n",
       "      <th>FC</th>\n",
       "      <th>USDKRW</th>\n",
       "      <th>IEF</th>\n",
       "      <th>TLT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>2323.76</td>\n",
       "      <td>65.095131</td>\n",
       "      <td>40.950489</td>\n",
       "      <td>86.671799</td>\n",
       "      <td>53.904892</td>\n",
       "      <td>29.433332</td>\n",
       "      <td>177.699997</td>\n",
       "      <td>82.003815</td>\n",
       "      <td>34.854122</td>\n",
       "      <td>104.637054</td>\n",
       "      <td>...</td>\n",
       "      <td>18.76</td>\n",
       "      <td>2161.0</td>\n",
       "      <td>190.08</td>\n",
       "      <td>63.77</td>\n",
       "      <td>193.820</td>\n",
       "      <td>121.182</td>\n",
       "      <td>240.288</td>\n",
       "      <td>1061.20</td>\n",
       "      <td>96.978477</td>\n",
       "      <td>113.163429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>2339.29</td>\n",
       "      <td>66.751396</td>\n",
       "      <td>40.943363</td>\n",
       "      <td>86.994362</td>\n",
       "      <td>54.024086</td>\n",
       "      <td>29.459999</td>\n",
       "      <td>181.039993</td>\n",
       "      <td>83.021141</td>\n",
       "      <td>34.584610</td>\n",
       "      <td>105.773712</td>\n",
       "      <td>...</td>\n",
       "      <td>18.78</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>188.00</td>\n",
       "      <td>64.18</td>\n",
       "      <td>194.250</td>\n",
       "      <td>122.113</td>\n",
       "      <td>240.206</td>\n",
       "      <td>1064.55</td>\n",
       "      <td>97.079865</td>\n",
       "      <td>113.704529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>2350.30</td>\n",
       "      <td>66.250656</td>\n",
       "      <td>41.133541</td>\n",
       "      <td>86.800812</td>\n",
       "      <td>53.932396</td>\n",
       "      <td>29.570000</td>\n",
       "      <td>183.220001</td>\n",
       "      <td>82.930305</td>\n",
       "      <td>35.167095</td>\n",
       "      <td>106.784096</td>\n",
       "      <td>...</td>\n",
       "      <td>18.72</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>189.36</td>\n",
       "      <td>65.05</td>\n",
       "      <td>193.391</td>\n",
       "      <td>123.206</td>\n",
       "      <td>238.120</td>\n",
       "      <td>1062.15</td>\n",
       "      <td>97.033798</td>\n",
       "      <td>113.686455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>2366.48</td>\n",
       "      <td>67.309898</td>\n",
       "      <td>41.601864</td>\n",
       "      <td>87.851501</td>\n",
       "      <td>54.088268</td>\n",
       "      <td>29.453333</td>\n",
       "      <td>185.339996</td>\n",
       "      <td>83.266373</td>\n",
       "      <td>34.932358</td>\n",
       "      <td>106.720947</td>\n",
       "      <td>...</td>\n",
       "      <td>18.55</td>\n",
       "      <td>2119.0</td>\n",
       "      <td>187.71</td>\n",
       "      <td>64.17</td>\n",
       "      <td>188.744</td>\n",
       "      <td>123.165</td>\n",
       "      <td>232.272</td>\n",
       "      <td>1062.75</td>\n",
       "      <td>96.914001</td>\n",
       "      <td>113.361839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>2370.14</td>\n",
       "      <td>67.454338</td>\n",
       "      <td>41.447346</td>\n",
       "      <td>89.307693</td>\n",
       "      <td>53.932396</td>\n",
       "      <td>29.456667</td>\n",
       "      <td>185.039993</td>\n",
       "      <td>83.411736</td>\n",
       "      <td>34.854122</td>\n",
       "      <td>106.396202</td>\n",
       "      <td>...</td>\n",
       "      <td>18.25</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>183.05</td>\n",
       "      <td>64.35</td>\n",
       "      <td>186.323</td>\n",
       "      <td>124.339</td>\n",
       "      <td>232.190</td>\n",
       "      <td>1066.10</td>\n",
       "      <td>96.867905</td>\n",
       "      <td>113.289734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-23</th>\n",
       "      <td>4801.71</td>\n",
       "      <td>156.562347</td>\n",
       "      <td>175.262802</td>\n",
       "      <td>127.924423</td>\n",
       "      <td>136.246506</td>\n",
       "      <td>43.480000</td>\n",
       "      <td>569.619995</td>\n",
       "      <td>169.451706</td>\n",
       "      <td>64.221039</td>\n",
       "      <td>237.479584</td>\n",
       "      <td>...</td>\n",
       "      <td>19.04</td>\n",
       "      <td>2591.0</td>\n",
       "      <td>219.98</td>\n",
       "      <td>86.08</td>\n",
       "      <td>156.671</td>\n",
       "      <td>70.424</td>\n",
       "      <td>193.477</td>\n",
       "      <td>1187.75</td>\n",
       "      <td>113.202599</td>\n",
       "      <td>145.104370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>4869.42</td>\n",
       "      <td>157.494995</td>\n",
       "      <td>179.289459</td>\n",
       "      <td>130.265030</td>\n",
       "      <td>138.498352</td>\n",
       "      <td>43.930000</td>\n",
       "      <td>577.679993</td>\n",
       "      <td>172.209473</td>\n",
       "      <td>64.839310</td>\n",
       "      <td>241.689438</td>\n",
       "      <td>...</td>\n",
       "      <td>19.06</td>\n",
       "      <td>2578.0</td>\n",
       "      <td>216.17</td>\n",
       "      <td>88.32</td>\n",
       "      <td>156.278</td>\n",
       "      <td>71.298</td>\n",
       "      <td>192.916</td>\n",
       "      <td>1186.80</td>\n",
       "      <td>113.241852</td>\n",
       "      <td>145.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>4865.61</td>\n",
       "      <td>157.931503</td>\n",
       "      <td>178.255417</td>\n",
       "      <td>130.719345</td>\n",
       "      <td>137.529083</td>\n",
       "      <td>44.270000</td>\n",
       "      <td>569.359985</td>\n",
       "      <td>171.139740</td>\n",
       "      <td>65.683281</td>\n",
       "      <td>241.473572</td>\n",
       "      <td>...</td>\n",
       "      <td>18.83</td>\n",
       "      <td>2603.0</td>\n",
       "      <td>214.79</td>\n",
       "      <td>86.88</td>\n",
       "      <td>156.418</td>\n",
       "      <td>71.156</td>\n",
       "      <td>195.220</td>\n",
       "      <td>1188.10</td>\n",
       "      <td>113.222221</td>\n",
       "      <td>144.879654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>4871.72</td>\n",
       "      <td>159.389984</td>\n",
       "      <td>178.344925</td>\n",
       "      <td>131.687180</td>\n",
       "      <td>138.233994</td>\n",
       "      <td>44.599998</td>\n",
       "      <td>569.289978</td>\n",
       "      <td>172.258560</td>\n",
       "      <td>65.761803</td>\n",
       "      <td>243.377304</td>\n",
       "      <td>...</td>\n",
       "      <td>18.98</td>\n",
       "      <td>2633.0</td>\n",
       "      <td>217.93</td>\n",
       "      <td>88.75</td>\n",
       "      <td>157.905</td>\n",
       "      <td>71.887</td>\n",
       "      <td>198.557</td>\n",
       "      <td>1186.40</td>\n",
       "      <td>112.643059</td>\n",
       "      <td>143.296921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>4859.24</td>\n",
       "      <td>159.618195</td>\n",
       "      <td>177.171738</td>\n",
       "      <td>132.111847</td>\n",
       "      <td>138.047974</td>\n",
       "      <td>44.330002</td>\n",
       "      <td>570.530029</td>\n",
       "      <td>171.532303</td>\n",
       "      <td>65.614594</td>\n",
       "      <td>240.973083</td>\n",
       "      <td>...</td>\n",
       "      <td>18.66</td>\n",
       "      <td>2675.0</td>\n",
       "      <td>217.79</td>\n",
       "      <td>89.61</td>\n",
       "      <td>157.063</td>\n",
       "      <td>71.379</td>\n",
       "      <td>199.680</td>\n",
       "      <td>1188.90</td>\n",
       "      <td>113.016075</td>\n",
       "      <td>144.498611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>956 rows × 389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            S&P 500           A        AAPL         ABC         ABT  \\\n",
       "2018-01-02  2323.76   65.095131   40.950489   86.671799   53.904892   \n",
       "2018-01-03  2339.29   66.751396   40.943363   86.994362   54.024086   \n",
       "2018-01-04  2350.30   66.250656   41.133541   86.800812   53.932396   \n",
       "2018-01-05  2366.48   67.309898   41.601864   87.851501   54.088268   \n",
       "2018-01-08  2370.14   67.454338   41.447346   89.307693   53.932396   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "2021-12-23  4801.71  156.562347  175.262802  127.924423  136.246506   \n",
       "2021-12-27  4869.42  157.494995  179.289459  130.265030  138.498352   \n",
       "2021-12-28  4865.61  157.931503  178.255417  130.719345  137.529083   \n",
       "2021-12-29  4871.72  159.389984  178.344925  131.687180  138.233994   \n",
       "2021-12-30  4859.24  159.618195  177.171738  132.111847  138.047974   \n",
       "\n",
       "                 ACGL        ADBE         ADI        ADM         ADP  ...  \\\n",
       "2018-01-02  29.433332  177.699997   82.003815  34.854122  104.637054  ...   \n",
       "2018-01-03  29.459999  181.039993   83.021141  34.584610  105.773712  ...   \n",
       "2018-01-04  29.570000  183.220001   82.930305  35.167095  106.784096  ...   \n",
       "2018-01-05  29.453333  185.339996   83.266373  34.932358  106.720947  ...   \n",
       "2018-01-08  29.456667  185.039993   83.411736  34.854122  106.396202  ...   \n",
       "...               ...         ...         ...        ...         ...  ...   \n",
       "2021-12-23  43.480000  569.619995  169.451706  64.221039  237.479584  ...   \n",
       "2021-12-27  43.930000  577.679993  172.209473  64.839310  241.689438  ...   \n",
       "2021-12-28  44.270000  569.359985  171.139740  65.683281  241.473572  ...   \n",
       "2021-12-29  44.599998  569.289978  172.258560  65.761803  243.377304  ...   \n",
       "2021-12-30  44.330002  570.530029  171.532303  65.614594  240.973083  ...   \n",
       "\n",
       "               SB      CC      KC     CT       LC     LH.1       FC   USDKRW  \\\n",
       "2018-01-02  18.76  2161.0  190.08  63.77  193.820  121.182  240.288  1061.20   \n",
       "2018-01-03  18.78  2130.0  188.00  64.18  194.250  122.113  240.206  1064.55   \n",
       "2018-01-04  18.72  2130.0  189.36  65.05  193.391  123.206  238.120  1062.15   \n",
       "2018-01-05  18.55  2119.0  187.71  64.17  188.744  123.165  232.272  1062.75   \n",
       "2018-01-08  18.25  2139.0  183.05  64.35  186.323  124.339  232.190  1066.10   \n",
       "...           ...     ...     ...    ...      ...      ...      ...      ...   \n",
       "2021-12-23  19.04  2591.0  219.98  86.08  156.671   70.424  193.477  1187.75   \n",
       "2021-12-27  19.06  2578.0  216.17  88.32  156.278   71.298  192.916  1186.80   \n",
       "2021-12-28  18.83  2603.0  214.79  86.88  156.418   71.156  195.220  1188.10   \n",
       "2021-12-29  18.98  2633.0  217.93  88.75  157.905   71.887  198.557  1186.40   \n",
       "2021-12-30  18.66  2675.0  217.79  89.61  157.063   71.379  199.680  1188.90   \n",
       "\n",
       "                   IEF         TLT  \n",
       "2018-01-02   96.978477  113.163429  \n",
       "2018-01-03   97.079865  113.704529  \n",
       "2018-01-04   97.033798  113.686455  \n",
       "2018-01-05   96.914001  113.361839  \n",
       "2018-01-08   96.867905  113.289734  \n",
       "...                ...         ...  \n",
       "2021-12-23  113.202599  145.104370  \n",
       "2021-12-27  113.241852  145.456100  \n",
       "2021-12-28  113.222221  144.879654  \n",
       "2021-12-29  112.643059  143.296921  \n",
       "2021-12-30  113.016075  144.498611  \n",
       "\n",
       "[956 rows x 389 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883e48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-01-02    2323.76\n",
       "2018-01-03    2339.29\n",
       "2018-01-04    2350.30\n",
       "2018-01-05    2366.48\n",
       "2018-01-08    2370.14\n",
       "               ...   \n",
       "2021-12-23    4801.71\n",
       "2021-12-27    4869.42\n",
       "2021-12-28    4865.61\n",
       "2021-12-29    4871.72\n",
       "2021-12-30    4859.24\n",
       "Name: S&P 500, Length: 956, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snp = price['S&P 500']\n",
    "snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4847631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "snp = min_max_scaler.fit_transform(snp.to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a8d03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snp = price.iloc[:, :1]\n",
    "# train = snp[:-30]\n",
    "# data_train = train.to_numpy()\n",
    "# data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f6b3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = snp[-30:]\n",
    "# test\n",
    "# data_test = test.to_numpy()\n",
    "# data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4923b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_scaler = MinMaxScaler()\n",
    "# data_train = min_max_scaler.fit_transform(train.to_numpy().reshape(-1,1))\n",
    "# data_test = min_max_scaler.transform(test.to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64d0efbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9476\\1936041625.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;31m# Create an instance of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnsembleModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;31m# Define the loss function and the optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9476\\1936041625.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, hidden_size, num_layers, num_heads, seq_len, output_size)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEnsembleModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9476\\1936041625.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, hidden_size, num_layers, num_heads, seq_len, output_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransformerEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, device, dtype)\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mfactory_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dtype'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n\u001b[0m\u001b[0;32m    405\u001b[0m                                             **factory_kwargs)\n\u001b[0;32m    406\u001b[0m         \u001b[1;31m# Implementation of Feedforward model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"embed_dim must be divisible by num_heads\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qkv_same_embed_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define the transformer model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, seq_len, output_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.seq_len = seq_len\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(input_size, hidden_size, num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(seq_len * hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.seq_len, self.input_size)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.view(-1, self.seq_len * self.hidden_size)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[-1])\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, seq_len, output_size):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.transformer = Transformer(input_size, hidden_size, num_layers, num_heads, seq_len, output_size)\n",
    "        self.lstm = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "        self.fc = nn.Linear(2 * output_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        transformer_output = self.transformer(x)\n",
    "        lstm_output = self.lstm(x)\n",
    "        combined_output = torch.cat((transformer_output, lstm_output), dim=1)\n",
    "        final_output = self.fc(combined_output)\n",
    "        return final_output\n",
    "    \n",
    "    \n",
    "# Load the stock price data into a Pandas dataframe\n",
    "df = pd.read_csv('../price_universe.csv', index_col=0)\n",
    "df.index = pd.to_datetime(price.index)\n",
    "\n",
    "# Convert the dataframe to a NumPy array\n",
    "data = snp\n",
    "\n",
    "# Normalize the data to be between 0 and 1\n",
    "data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "training_data = data[:int(0.8 * len(data))]\n",
    "validation_data = data[int(0.8 * len(data)):]\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "training_data = torch.tensor(training_data, dtype=torch.float32)\n",
    "validation_data = torch.tensor(validation_data, dtype=torch.float32)\n",
    "\n",
    "# Define the hyperparameters for the model\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_heads = 8\n",
    "seq_len = 30\n",
    "output_size = 1\n",
    "\n",
    "# Create an instance of the model\n",
    "model = EnsembleModel(input_size, hidden_size, num_layers, num_heads, seq_len, output_size)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(training_data.shape[0] - seq_len):\n",
    "        inputs = training_data[i:i+seq_len].unsqueeze(0)\n",
    "        targets = training_data[i+seq_len].unsqueeze(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Epoch: {}/{}, Loss: {:.4f}\".format(epoch+1, num_epochs, loss.item()))\n",
    "    \n",
    "# Evaluate the model on the validation set\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    num_predictions = 0\n",
    "    for i in range(validation_data.shape[0] - seq_len):\n",
    "        inputs = validation_data[i:i+seq_len].unsqueeze(0)\n",
    "        targets = validation_data[i+seq_len].unsqueeze(0)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "        num_predictions += 1\n",
    "        \n",
    "    average_loss = total_loss / num_predictions\n",
    "    print(\"Validation Loss: {:.4f}\".format(average_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad199c73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9476\\533006554.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not determine the shape of object type 'DataFrame'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the number of inputs, hidden units, and outputs\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden=None):\n",
    "        lstm_out, hidden = self.lstm(input.view(len(input), 1, -1), hidden)\n",
    "        output = self.linear(lstm_out.view(len(input), -1))\n",
    "        return output, hidden\n",
    "\n",
    "# Load the S&P 500 data\n",
    "\n",
    "df = pd.read_csv('../price_universe.csv', index_col=0)\n",
    "df.index = pd.to_datetime(price.index)\n",
    "\n",
    "# data = np.loadtxt('../price_universe.csv', delimiter=',')\n",
    "\n",
    "# Normalize the data\n",
    "data = (df - np.mean(data)) / np.std(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "training_data = data[:int(len(data) * 0.8)]\n",
    "test_data = data[int(len(data) * 0.8):]\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create the LSTM model\n",
    "model = LSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = (torch.zeros(1, 1, hidden_size),\n",
    "              torch.zeros(1, 1, hidden_size))\n",
    "    \n",
    "    for i in range(len(training_data) - output_size):\n",
    "        input = torch.tensor([training_data[i:i + input_size]])\n",
    "        target = torch.tensor([training_data[i + input_size:i + input_size + output_size]])\n",
    "        \n",
    "        output, hidden = model(input, hidden)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Test the model\n",
    "hidden = (torch.zeros(1, 1, hidden_size),\n",
    "          torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(test_data) - input_size):\n",
    "    input = torch.tensor([test_data[i:i + input_size]])\n",
    "    output, hidden = model(input, hidden)\n",
    "    predictions.append\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a965285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S&amp;P 500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>2323.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>2339.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>2350.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>2366.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>2370.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-23</th>\n",
       "      <td>4801.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>4869.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>4865.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>4871.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>4859.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>956 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            S&P 500\n",
       "2018-01-02  2323.76\n",
       "2018-01-03  2339.29\n",
       "2018-01-04  2350.30\n",
       "2018-01-05  2366.48\n",
       "2018-01-08  2370.14\n",
       "...             ...\n",
       "2021-12-23  4801.71\n",
       "2021-12-27  4869.42\n",
       "2021-12-28  4865.61\n",
       "2021-12-29  4871.72\n",
       "2021-12-30  4859.24\n",
       "\n",
       "[956 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../price_universe.csv', index_col=0)\n",
    "df.index = pd.to_datetime(price.index)\n",
    "df = df.iloc[:, :1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780fe08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bongkyun\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([926, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Loss: 1.001600980758667\n",
      "Epoch [40/200], Loss: 1.0004600286483765\n",
      "Epoch [60/200], Loss: 1.0001320838928223\n",
      "Epoch [80/200], Loss: 1.0000379085540771\n",
      "Epoch [100/200], Loss: 1.0000109672546387\n",
      "Epoch [120/200], Loss: 1.0000032186508179\n",
      "Epoch [140/200], Loss: 1.0000009536743164\n",
      "Epoch [160/200], Loss: 1.000000238418579\n",
      "Epoch [180/200], Loss: 1.0000001192092896\n",
      "Epoch [200/200], Loss: 1.0000001192092896\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12024\\350947964.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;31m# Evaluate the model on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mtest_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Test Loss: {test_loss.item()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12024\\350947964.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the S&P 500 data\n",
    "df = pd.read_csv('../price_universe.csv', index_col=0)\n",
    "df.index = pd.to_datetime(price.index)\n",
    "df = df.iloc[:, :1]\n",
    "\n",
    "# Preprocess the data\n",
    "data = df.values\n",
    "data = data.astype(\"float32\")\n",
    "data = data.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data = data[:-30]\n",
    "test_data = data[-30:]\n",
    "\n",
    "# Scale the data to have zero mean and unit variance\n",
    "mean = train_data.mean()\n",
    "std = train_data.std()\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "\n",
    "# Convert the data to tensors\n",
    "train_data = torch.from_numpy(train_data)\n",
    "test_data = torch.from_numpy(test_data)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 200\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(train_data)\n",
    "#     loss = criterion(outputs, train_data)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     if (epoch + 1) % 20 == 0:\n",
    "#         print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        \n",
    "        \n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    inputs = train_data.unsqueeze(0) # add a batch dimension\n",
    "    outputs = model(inputs)\n",
    "#     loss = criterion(outputs, train_data)\n",
    "    loss = criterion(outputs.squeeze(0), train_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_data)\n",
    "    test_loss = criterion(test_outputs, test_data)\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "# Invert the scaling on the test outputs to obtain the predicted values\n",
    "predicted = test_outputs.reshape(-1).detach().numpy() * std + mean\n",
    "\n",
    "# Plot the predicted values along with the actual values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(test_data)), predicted, label='Predicted')\n",
    "plt.plot(range(len(test_data)), test_data.detach().numpy() * std + mean, label='Actual')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.title(\"Predicted vs Actual Stock Prices\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64faf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
