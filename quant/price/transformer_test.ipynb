{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81ee890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# import pickle5 as pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a02be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.read_csv('../price_universe.csv', index_col=0)\n",
    "price.index = pd.to_datetime(price.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a07071d8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>A</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACGL</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>...</th>\n",
       "      <th>SB</th>\n",
       "      <th>CC</th>\n",
       "      <th>KC</th>\n",
       "      <th>CT</th>\n",
       "      <th>LC</th>\n",
       "      <th>LH.1</th>\n",
       "      <th>FC</th>\n",
       "      <th>USDKRW</th>\n",
       "      <th>IEF</th>\n",
       "      <th>TLT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>2323.76</td>\n",
       "      <td>65.095131</td>\n",
       "      <td>40.950489</td>\n",
       "      <td>86.671799</td>\n",
       "      <td>53.904892</td>\n",
       "      <td>29.433332</td>\n",
       "      <td>177.699997</td>\n",
       "      <td>82.003815</td>\n",
       "      <td>34.854122</td>\n",
       "      <td>104.637054</td>\n",
       "      <td>...</td>\n",
       "      <td>18.76</td>\n",
       "      <td>2161.0</td>\n",
       "      <td>190.08</td>\n",
       "      <td>63.77</td>\n",
       "      <td>193.820</td>\n",
       "      <td>121.182</td>\n",
       "      <td>240.288</td>\n",
       "      <td>1061.20</td>\n",
       "      <td>96.978477</td>\n",
       "      <td>113.163429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>2339.29</td>\n",
       "      <td>66.751396</td>\n",
       "      <td>40.943363</td>\n",
       "      <td>86.994362</td>\n",
       "      <td>54.024086</td>\n",
       "      <td>29.459999</td>\n",
       "      <td>181.039993</td>\n",
       "      <td>83.021141</td>\n",
       "      <td>34.584610</td>\n",
       "      <td>105.773712</td>\n",
       "      <td>...</td>\n",
       "      <td>18.78</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>188.00</td>\n",
       "      <td>64.18</td>\n",
       "      <td>194.250</td>\n",
       "      <td>122.113</td>\n",
       "      <td>240.206</td>\n",
       "      <td>1064.55</td>\n",
       "      <td>97.079865</td>\n",
       "      <td>113.704529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>2350.30</td>\n",
       "      <td>66.250656</td>\n",
       "      <td>41.133541</td>\n",
       "      <td>86.800812</td>\n",
       "      <td>53.932396</td>\n",
       "      <td>29.570000</td>\n",
       "      <td>183.220001</td>\n",
       "      <td>82.930305</td>\n",
       "      <td>35.167095</td>\n",
       "      <td>106.784096</td>\n",
       "      <td>...</td>\n",
       "      <td>18.72</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>189.36</td>\n",
       "      <td>65.05</td>\n",
       "      <td>193.391</td>\n",
       "      <td>123.206</td>\n",
       "      <td>238.120</td>\n",
       "      <td>1062.15</td>\n",
       "      <td>97.033798</td>\n",
       "      <td>113.686455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>2366.48</td>\n",
       "      <td>67.309898</td>\n",
       "      <td>41.601864</td>\n",
       "      <td>87.851501</td>\n",
       "      <td>54.088268</td>\n",
       "      <td>29.453333</td>\n",
       "      <td>185.339996</td>\n",
       "      <td>83.266373</td>\n",
       "      <td>34.932358</td>\n",
       "      <td>106.720947</td>\n",
       "      <td>...</td>\n",
       "      <td>18.55</td>\n",
       "      <td>2119.0</td>\n",
       "      <td>187.71</td>\n",
       "      <td>64.17</td>\n",
       "      <td>188.744</td>\n",
       "      <td>123.165</td>\n",
       "      <td>232.272</td>\n",
       "      <td>1062.75</td>\n",
       "      <td>96.914001</td>\n",
       "      <td>113.361839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>2370.14</td>\n",
       "      <td>67.454338</td>\n",
       "      <td>41.447346</td>\n",
       "      <td>89.307693</td>\n",
       "      <td>53.932396</td>\n",
       "      <td>29.456667</td>\n",
       "      <td>185.039993</td>\n",
       "      <td>83.411736</td>\n",
       "      <td>34.854122</td>\n",
       "      <td>106.396202</td>\n",
       "      <td>...</td>\n",
       "      <td>18.25</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>183.05</td>\n",
       "      <td>64.35</td>\n",
       "      <td>186.323</td>\n",
       "      <td>124.339</td>\n",
       "      <td>232.190</td>\n",
       "      <td>1066.10</td>\n",
       "      <td>96.867905</td>\n",
       "      <td>113.289734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-23</th>\n",
       "      <td>4801.71</td>\n",
       "      <td>156.562347</td>\n",
       "      <td>175.262802</td>\n",
       "      <td>127.924423</td>\n",
       "      <td>136.246506</td>\n",
       "      <td>43.480000</td>\n",
       "      <td>569.619995</td>\n",
       "      <td>169.451706</td>\n",
       "      <td>64.221039</td>\n",
       "      <td>237.479584</td>\n",
       "      <td>...</td>\n",
       "      <td>19.04</td>\n",
       "      <td>2591.0</td>\n",
       "      <td>219.98</td>\n",
       "      <td>86.08</td>\n",
       "      <td>156.671</td>\n",
       "      <td>70.424</td>\n",
       "      <td>193.477</td>\n",
       "      <td>1187.75</td>\n",
       "      <td>113.202599</td>\n",
       "      <td>145.104370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>4869.42</td>\n",
       "      <td>157.494995</td>\n",
       "      <td>179.289459</td>\n",
       "      <td>130.265030</td>\n",
       "      <td>138.498352</td>\n",
       "      <td>43.930000</td>\n",
       "      <td>577.679993</td>\n",
       "      <td>172.209473</td>\n",
       "      <td>64.839310</td>\n",
       "      <td>241.689438</td>\n",
       "      <td>...</td>\n",
       "      <td>19.06</td>\n",
       "      <td>2578.0</td>\n",
       "      <td>216.17</td>\n",
       "      <td>88.32</td>\n",
       "      <td>156.278</td>\n",
       "      <td>71.298</td>\n",
       "      <td>192.916</td>\n",
       "      <td>1186.80</td>\n",
       "      <td>113.241852</td>\n",
       "      <td>145.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>4865.61</td>\n",
       "      <td>157.931503</td>\n",
       "      <td>178.255417</td>\n",
       "      <td>130.719345</td>\n",
       "      <td>137.529083</td>\n",
       "      <td>44.270000</td>\n",
       "      <td>569.359985</td>\n",
       "      <td>171.139740</td>\n",
       "      <td>65.683281</td>\n",
       "      <td>241.473572</td>\n",
       "      <td>...</td>\n",
       "      <td>18.83</td>\n",
       "      <td>2603.0</td>\n",
       "      <td>214.79</td>\n",
       "      <td>86.88</td>\n",
       "      <td>156.418</td>\n",
       "      <td>71.156</td>\n",
       "      <td>195.220</td>\n",
       "      <td>1188.10</td>\n",
       "      <td>113.222221</td>\n",
       "      <td>144.879654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>4871.72</td>\n",
       "      <td>159.389984</td>\n",
       "      <td>178.344925</td>\n",
       "      <td>131.687180</td>\n",
       "      <td>138.233994</td>\n",
       "      <td>44.599998</td>\n",
       "      <td>569.289978</td>\n",
       "      <td>172.258560</td>\n",
       "      <td>65.761803</td>\n",
       "      <td>243.377304</td>\n",
       "      <td>...</td>\n",
       "      <td>18.98</td>\n",
       "      <td>2633.0</td>\n",
       "      <td>217.93</td>\n",
       "      <td>88.75</td>\n",
       "      <td>157.905</td>\n",
       "      <td>71.887</td>\n",
       "      <td>198.557</td>\n",
       "      <td>1186.40</td>\n",
       "      <td>112.643059</td>\n",
       "      <td>143.296921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>4859.24</td>\n",
       "      <td>159.618195</td>\n",
       "      <td>177.171738</td>\n",
       "      <td>132.111847</td>\n",
       "      <td>138.047974</td>\n",
       "      <td>44.330002</td>\n",
       "      <td>570.530029</td>\n",
       "      <td>171.532303</td>\n",
       "      <td>65.614594</td>\n",
       "      <td>240.973083</td>\n",
       "      <td>...</td>\n",
       "      <td>18.66</td>\n",
       "      <td>2675.0</td>\n",
       "      <td>217.79</td>\n",
       "      <td>89.61</td>\n",
       "      <td>157.063</td>\n",
       "      <td>71.379</td>\n",
       "      <td>199.680</td>\n",
       "      <td>1188.90</td>\n",
       "      <td>113.016075</td>\n",
       "      <td>144.498611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>956 rows Ã— 389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            S&P 500           A        AAPL         ABC         ABT  \\\n",
       "2018-01-02  2323.76   65.095131   40.950489   86.671799   53.904892   \n",
       "2018-01-03  2339.29   66.751396   40.943363   86.994362   54.024086   \n",
       "2018-01-04  2350.30   66.250656   41.133541   86.800812   53.932396   \n",
       "2018-01-05  2366.48   67.309898   41.601864   87.851501   54.088268   \n",
       "2018-01-08  2370.14   67.454338   41.447346   89.307693   53.932396   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "2021-12-23  4801.71  156.562347  175.262802  127.924423  136.246506   \n",
       "2021-12-27  4869.42  157.494995  179.289459  130.265030  138.498352   \n",
       "2021-12-28  4865.61  157.931503  178.255417  130.719345  137.529083   \n",
       "2021-12-29  4871.72  159.389984  178.344925  131.687180  138.233994   \n",
       "2021-12-30  4859.24  159.618195  177.171738  132.111847  138.047974   \n",
       "\n",
       "                 ACGL        ADBE         ADI        ADM         ADP  ...  \\\n",
       "2018-01-02  29.433332  177.699997   82.003815  34.854122  104.637054  ...   \n",
       "2018-01-03  29.459999  181.039993   83.021141  34.584610  105.773712  ...   \n",
       "2018-01-04  29.570000  183.220001   82.930305  35.167095  106.784096  ...   \n",
       "2018-01-05  29.453333  185.339996   83.266373  34.932358  106.720947  ...   \n",
       "2018-01-08  29.456667  185.039993   83.411736  34.854122  106.396202  ...   \n",
       "...               ...         ...         ...        ...         ...  ...   \n",
       "2021-12-23  43.480000  569.619995  169.451706  64.221039  237.479584  ...   \n",
       "2021-12-27  43.930000  577.679993  172.209473  64.839310  241.689438  ...   \n",
       "2021-12-28  44.270000  569.359985  171.139740  65.683281  241.473572  ...   \n",
       "2021-12-29  44.599998  569.289978  172.258560  65.761803  243.377304  ...   \n",
       "2021-12-30  44.330002  570.530029  171.532303  65.614594  240.973083  ...   \n",
       "\n",
       "               SB      CC      KC     CT       LC     LH.1       FC   USDKRW  \\\n",
       "2018-01-02  18.76  2161.0  190.08  63.77  193.820  121.182  240.288  1061.20   \n",
       "2018-01-03  18.78  2130.0  188.00  64.18  194.250  122.113  240.206  1064.55   \n",
       "2018-01-04  18.72  2130.0  189.36  65.05  193.391  123.206  238.120  1062.15   \n",
       "2018-01-05  18.55  2119.0  187.71  64.17  188.744  123.165  232.272  1062.75   \n",
       "2018-01-08  18.25  2139.0  183.05  64.35  186.323  124.339  232.190  1066.10   \n",
       "...           ...     ...     ...    ...      ...      ...      ...      ...   \n",
       "2021-12-23  19.04  2591.0  219.98  86.08  156.671   70.424  193.477  1187.75   \n",
       "2021-12-27  19.06  2578.0  216.17  88.32  156.278   71.298  192.916  1186.80   \n",
       "2021-12-28  18.83  2603.0  214.79  86.88  156.418   71.156  195.220  1188.10   \n",
       "2021-12-29  18.98  2633.0  217.93  88.75  157.905   71.887  198.557  1186.40   \n",
       "2021-12-30  18.66  2675.0  217.79  89.61  157.063   71.379  199.680  1188.90   \n",
       "\n",
       "                   IEF         TLT  \n",
       "2018-01-02   96.978477  113.163429  \n",
       "2018-01-03   97.079865  113.704529  \n",
       "2018-01-04   97.033798  113.686455  \n",
       "2018-01-05   96.914001  113.361839  \n",
       "2018-01-08   96.867905  113.289734  \n",
       "...                ...         ...  \n",
       "2021-12-23  113.202599  145.104370  \n",
       "2021-12-27  113.241852  145.456100  \n",
       "2021-12-28  113.222221  144.879654  \n",
       "2021-12-29  112.643059  143.296921  \n",
       "2021-12-30  113.016075  144.498611  \n",
       "\n",
       "[956 rows x 389 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "883e48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-01-02    2323.76\n",
       "2018-01-03    2339.29\n",
       "2018-01-04    2350.30\n",
       "2018-01-05    2366.48\n",
       "2018-01-08    2370.14\n",
       "               ...   \n",
       "2021-12-23    4801.71\n",
       "2021-12-27    4869.42\n",
       "2021-12-28    4865.61\n",
       "2021-12-29    4871.72\n",
       "2021-12-30    4859.24\n",
       "Name: S&P 500, Length: 956, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snp = price['S&P 500']\n",
    "snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4847631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "snp = min_max_scaler.fit_transform(snp.to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a8d03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snp = price.iloc[:, :1]\n",
    "# train = snp[:-30]\n",
    "# data_train = train.to_numpy()\n",
    "# data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f6b3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = snp[-30:]\n",
    "# test\n",
    "# data_test = test.to_numpy()\n",
    "# data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4923b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_scaler = MinMaxScaler()\n",
    "# data_train = min_max_scaler.fit_transform(train.to_numpy().reshape(-1,1))\n",
    "# data_test = min_max_scaler.transform(test.to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad99f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Transformer Model\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, ffn_hidden_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(input_size, num_heads, ffn_hidden_size, dropout_rate)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = n # n is the number of features in your dataset\n",
    "hidden_size = 128\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "ffn_hidden_size = 128\n",
    "dropout_rate = 0.1\n",
    "model = StockTransformer(input_size, hidden_size, num_layers, num_heads, ffn_hidden_size, dropout_rate)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f9f842",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Transformer' from 'tensorflow.keras.layers' (C:\\Users\\bongkyun\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\api\\_v2\\keras\\layers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\2791814807.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Transformer' from 'tensorflow.keras.layers' (C:\\Users\\bongkyun\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\api\\_v2\\keras\\layers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Transformer, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the S&P 500 data into a pandas DataFrame\n",
    "data = pd.read_csv('../price_universe.csv', index_col=0)\n",
    "\n",
    "# Prepare the input data for the model\n",
    "input_data = price['S&P 500']\n",
    "\n",
    "# Define the Transformer model\n",
    "input_layer = Input(shape=(input_data.shape[1],))\n",
    "transformer = Transformer(num_heads=8, d_model=256, dff=1024, num_layers=2, activation=\"relu\")(input_layer)\n",
    "output_layer = Dense(units=1, activation=\"linear\")(transformer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Train the model on the input data\n",
    "model.fit(input_data, data[\"Close\"], epochs=100, batch_size=32)\n",
    "\n",
    "# Use the trained model to make predictions on new data\n",
    "new_data = np.array([[100, 102, 98, 101]])\n",
    "prediction = model.predict(new_data)\n",
    "print(\"The predicted stock price for the new data is:\", prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15f9ea06",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\2476796090.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Create the Transformer model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformerModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Define the loss function and optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\2476796090.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_layers, nhead, dim_model, dim_feedforward)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, activation, custom_encoder, custom_decoder, layer_norm_eps, batch_first, norm_first, device, dtype)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n\u001b[0m\u001b[0;32m     64\u001b[0m                                                     \u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_norm_eps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_first\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                                                     **factory_kwargs)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, device, dtype)\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mfactory_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dtype'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n\u001b[0m\u001b[0;32m    405\u001b[0m                                             **factory_kwargs)\n\u001b[0;32m    406\u001b[0m         \u001b[1;31m# Implementation of Feedforward model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"embed_dim must be divisible by num_heads\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qkv_same_embed_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the model architecture\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_layers, nhead, dim_model, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Transformer(n_layers, nhead, dim_model, dim_feedforward)\n",
    "        self.fc = nn.Linear(dim_model, 1)\n",
    "        \n",
    "    def forward(self, src, mask=None):\n",
    "        output = self.transformer(src, src, src, mask=mask)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Load the S&P 500 index data\n",
    "# Note: In practice, you would load the data using a library such as pandas\n",
    "sp500 = price['S&P 500']# Your S&P 500 index data\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = sp500[:-30]\n",
    "test_data = sp500[-30:]\n",
    "\n",
    "# Convert the data to tensors and normalize it\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_data = (train_data - train_data.mean()) / train_data.std()\n",
    "\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_data = (test_data - test_data.mean()) / test_data.std()\n",
    "\n",
    "# Create the Transformer model\n",
    "model = TransformerModel(n_layers=6, nhead=8, dim_model=256, dim_feedforward=1024)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Make a prediction on the training data\n",
    "    pred = model(train_data[:-1].unsqueeze(0))\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(pred, train_data[1:].unsqueeze(0))\n",
    "    \n",
    "    # Backpropagate the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}/100, Loss: {loss.item()}\")\n",
    "\n",
    "# Make a prediction on the test data\n",
    "with torch.no_grad():\n",
    "    test_pred = model(test_data[:-1].unsqueeze(0))\n",
    "\n",
    "# Calculate the MSE on the test data\n",
    "test_loss = criterion(test_pred, test_data[1:].unsqueeze(0))\n",
    "print(f\"Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a048d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\400616453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# Make a prediction on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# Calculate the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\400616453.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the model architecture\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, nhead, dim_model, num_encoder_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Transformer(dim_model, nhead, num_encoder_layers, dim_feedforward)\n",
    "        self.fc = nn.Linear(dim_model, 1)\n",
    "        \n",
    "#     def forward(self, src, mask=None):\n",
    "#         output = self.transformer(src, src_key_padding_mask=mask).mean(dim=1)\n",
    "#         output = self.fc(output[:, -1, :])\n",
    "#         return output\n",
    "    \n",
    "    def forward(self, src, mask=None):\n",
    "        src = src.transpose(0, 1)\n",
    "        output = self.transformer(src, src_key_padding_mask=mask).mean(dim=0)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Load the S&P 500 index data\n",
    "# Note: In practice, you would load the data using a library such as pandas\n",
    "sp500 = price['S&P 500']# Your S&P 500 index data\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = sp500[:-30]\n",
    "test_data = sp500[-30:]\n",
    "\n",
    "# Convert the data to tensors and normalize it\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_data = (train_data - train_data.mean()) / train_data.std()\n",
    "\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_data = (test_data - test_data.mean()) / test_data.std()\n",
    "\n",
    "# Create the Transformer model\n",
    "model = TransformerModel(nhead=8, dim_model=256, num_encoder_layers=6, dim_feedforward=1024)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Make a prediction on the training data\n",
    "    pred = model(train_data[:-1].unsqueeze(0))\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(pred, train_data[1:].unsqueeze(0))\n",
    "    \n",
    "    # Backpropagate the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}/100, Loss: {loss.item()}\")\n",
    "\n",
    "# Make a prediction on the test data\n",
    "with torch.no_grad():\n",
    "    test_pred = model(test_data[:-1].unsqueeze(0))\n",
    "\n",
    "# Calculate the MSE on the test data\n",
    "test_loss = criterion(test_pred, test_data[1:].unsqueeze(0))\n",
    "print(f\"Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dec8044",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 256, but got 925",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\2204099977.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Make a prediction on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# Calculate the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\2204099977.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    536\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    544\u001b[0m     def _sa_block(self, x: Tensor,\n\u001b[0;32m    545\u001b[0m                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n\u001b[1;32m--> 546\u001b[1;33m         x = self.self_attn(x, x, x,\n\u001b[0m\u001b[0;32m    547\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   1165\u001b[0m                 v_proj_weight=self.v_proj_weight, average_attn_weights=average_attn_weights)\n\u001b[0;32m   1166\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[0;32m   1168\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   5024\u001b[0m             raise AssertionError(\n\u001b[0;32m   5025\u001b[0m                 \"only bool and floating types of key_padding_mask are supported\")\n\u001b[1;32m-> 5026\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5027\u001b[0m         \u001b[1;34mf\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5028\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: was expecting embedding dimension of 256, but got 925"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the model architecture\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, nhead, dim_model, num_encoder_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(dim_model, nhead), num_encoder_layers)\n",
    "        self.fc = nn.Linear(dim_model, 1)\n",
    "        \n",
    "    def forward(self, src, mask=None):\n",
    "        output = self.transformer(src, src_key_padding_mask=mask).mean(dim=0)\n",
    "        output = self.fc(output).squeeze(-1)\n",
    "        return output\n",
    "\n",
    "# Load the S&P 500 index data\n",
    "# Note: In practice, you would load the data using a library such as pandas\n",
    "sp500 = price['S&P 500']# Your S&P 500 index data\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = sp500[:-30]\n",
    "test_data = sp500[-30:]\n",
    "\n",
    "# Convert the data to tensors and normalize it\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_data = (train_data - train_data.mean()) / train_data.std()\n",
    "\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_data = (test_data - test_data.mean()) / test_data.std()\n",
    "\n",
    "# Create the Transformer model\n",
    "model = TransformerModel(nhead=8, dim_model=256, num_encoder_layers=6, dim_feedforward=1024)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Make a prediction on the training data\n",
    "    pred = model(train_data[:-1].unsqueeze(0))\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(pred, train_data[1:])\n",
    "    \n",
    "    # Backpropagate the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}/100, Loss: {loss.item()}\")\n",
    "\n",
    "# Make a prediction on the test data\n",
    "with torch.no_grad():\n",
    "    test_pred = model(test_data[:-1].unsqueeze(0))\n",
    "\n",
    "# Calculate the MSE on the test data\n",
    "test_loss = criterion(test_pred, test_data[1:])\n",
    "print(f\"Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd78cb04",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\733998614.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# Make a prediction on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m# Calculate the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\733998614.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the model architecture\n",
    "# class TransformerModel(nn.Module):\n",
    "#     def __init__(self, nhead, dim_model, num_encoder_layers, dim_feedforward):\n",
    "#         super().__init__()\n",
    "#         self.transformer = nn.Transformer(dim_model, nhead, num_encoder_layers, dim_feedforward)\n",
    "#         self.fc = nn.Linear(dim_model, 1)\n",
    "        \n",
    "#     def forward(self, src, mask=None):\n",
    "#         src = src.unsqueeze(0)\n",
    "#         output = self.transformer(src, src_key_padding_mask=mask)\n",
    "#         output = self.fc(output[:, -1, :])\n",
    "#         return output\n",
    "    \n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, nhead, dim_model, num_encoder_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Transformer(dim_model, nhead, num_encoder_layers, dim_feedforward)\n",
    "        self.fc = nn.Linear(dim_model, 1)\n",
    "        \n",
    "    def forward(self, src, mask=None):\n",
    "        src = src.unsqueeze(0).permute(0, 2, 1)\n",
    "        output = self.transformer(src, src_key_padding_mask=mask).permute(0, 2, 1)\n",
    "        output = self.fc(output[:, -1, :]).squeeze(0)\n",
    "        return output\n",
    "\n",
    "# Load the S&P 500 index data\n",
    "# Note: In practice, you would load the data using a library such as pandas\n",
    "sp500 = price['S&P 500']# Your S&P 500 index data\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = sp500[:-30]\n",
    "test_data = sp500[-30:]\n",
    "\n",
    "# Convert the data to tensors and normalize it\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_data = (train_data - train_data.mean()) / train_data.std()\n",
    "\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_data = (test_data - test_data.mean()) / test_data.std()\n",
    "\n",
    "# Create the Transformer model\n",
    "model = TransformerModel(nhead=8, dim_model=256, num_encoder_layers=6, dim_feedforward=1024)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Make a prediction on the training data\n",
    "    pred = model(train_data[:-1])\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(pred, train_data[1:].unsqueeze(0))\n",
    "    \n",
    "    # Backpropagate the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}/100, Loss: {loss.item()}\")\n",
    "\n",
    "# Make a prediction on the test data\n",
    "with torch.no_grad():\n",
    "    test_pred = model(test_data[:-1])\n",
    "\n",
    "# Calculate the MSE on the test data\n",
    "test_loss = criterion(test_pred, test_data[1:].unsqueeze(0))\n",
    "print(f\"Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b40da239",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\768717364.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# Make a prediction on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# Calculate the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10344\\768717364.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the model architecture\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, nhead, dim_model, num_encoder_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Transformer(dim_model, nhead, num_encoder_layers, dim_feedforward)\n",
    "        self.fc = nn.Linear(dim_model, 1)\n",
    "        \n",
    "    def forward(self, src, mask=None):\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(src).bool()\n",
    "        src = src.unsqueeze(0).permute(0, 2, 1)\n",
    "        output = self.transformer(src, src_key_padding_mask=mask).permute(0, 2, 1)\n",
    "        output = self.fc(output[:, -1, :]).squeeze(0)\n",
    "        return output\n",
    "\n",
    "# Load the S&P 500 index data\n",
    "sp500 = price['S&P 500'] # Your S&P 500 index data\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = sp500[:-30]\n",
    "test_data = sp500[-30:]\n",
    "\n",
    "# Convert the data to tensors and normalize it\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_data = (train_data - train_data.mean()) / train_data.std()\n",
    "\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_data = (test_data - test_data.mean()) / test_data.std()\n",
    "\n",
    "# Create the Transformer model\n",
    "model = TransformerModel(nhead=8, dim_model=256, num_encoder_layers=6, dim_feedforward=1024)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Make a prediction on the training data\n",
    "    pred = model(train_data[:-1].unsqueeze(0))\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(pred, train_data[1:].unsqueeze(0))\n",
    "    \n",
    "    # Backpropagate the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}/100, Loss: {loss.item()}\")\n",
    "        \n",
    "# Make a prediction on the test data\n",
    "with torch.no_grad():\n",
    "    test_pred = model(test_data[:-1])\n",
    "\n",
    "# Calculate the MSE on the test data\n",
    "test_loss = criterion(test_pred, test_data[1:].unsqueeze(0))\n",
    "print(f\"Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0efbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
